[project]
name = "lm-eval-server"
version = "0.4.4"
description = "LM-Evaluation-Harness Server"
authors = [
    { name = "Rob Geada" },
    { name = "TrustyAI team" },
]
requires-python = "~=3.11"
readme = "README.md"
dependencies = [
    "lm-eval[api]==0.4.4",
    "fastapi>=0.115.9,<0.116",
    "pydantic>=2.4.2,<3",
    "uvicorn>=0.34.0,<0.35",
    "protobuf>=4.24.4,<5",
    "requests>=2.31.0,<3",
    "cryptography>=44.0.2,<45",
]

[dependency-groups]
dev = [
    "pytest>=7.4.2,<8",
    "isort>=5.12.0,<6",
    "flake8>=6.1.0,<7",
    "mypy>=1.5.1,<2",
    "pytest-cov>=4.1.0,<5",
    "httpx>=0.25.0,<0.26",
]

[tool.hatch.build.targets.sdist]
include = ["src"]

[tool.hatch.build.targets.wheel]
include = ["src"]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.isort]
profile = "black"
line_length = 88

[tool.mypy]
python_version = "3.11"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
no_implicit_optional = true
show_error_codes = true
warn_unused_ignores = true

[tool.ruff]
preview = true
line-length = 120
fix = true
output-format = "grouped"
extend-exclude = ["utilities/manifests"]

[tool.ruff.format]
exclude = [".git", ".venv", ".mypy_cache", "__pycache__"]

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = "test_*.py"
python_functions = "test_*"
